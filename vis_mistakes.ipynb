{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboardX'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mema\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelEma\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptim_factory\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_optimizer, get_parameter_groups, LayerDecayValueAssigner\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NativeScalerWithGradNormCount \u001B[38;5;28;01mas\u001B[39;00m NativeScaler\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbuild_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_dataset\n",
      "File \u001B[0;32m~/Desktop/jonathan/projects/Zero_Shot/MUST/utils.py:19\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_six\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m inf\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorboardX\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SummaryWriter\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mSmoothedValue\u001B[39;00m(\u001B[38;5;28mobject\u001B[39m):\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;124;03m\"\"\"Track a series of values and provide access to smoothed values over a\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m    window or the global series average.\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorboardX'"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import json\n",
    "import os\n",
    "from contextlib import suppress\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "from ema import ModelEma\n",
    "from optim_factory import create_optimizer, get_parameter_groups, LayerDecayValueAssigner\n",
    "\n",
    "import utils\n",
    "from utils import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "from build_dataset import build_dataset\n",
    "from engine_self_training import train_one_epoch, evaluate\n",
    "\n",
    "from model import clip_classifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser('MUST training and evaluation script', add_help=False)\n",
    "    parser.add_argument('--batch_size', default=64, type=int)\n",
    "    parser.add_argument('--save_ckpt_freq', default=10, type=int)\n",
    "    parser.add_argument('--eval_freq', default=1, type=int)\n",
    "\n",
    "    # CLIP parameters\n",
    "    parser.add_argument(\"--template\", default='templates.json', type=str)\n",
    "    parser.add_argument(\"--classname\", default='classes.json', type=str)\n",
    "    parser.add_argument('--clip_model', default='ViT-B/16', help='pretrained clip model name')\n",
    "    parser.add_argument('--image_mean', default=(0.48145466, 0.4578275, 0.40821073))\n",
    "    parser.add_argument('--image_std', default=(0.26862954, 0.26130258, 0.27577711))\n",
    "    parser.add_argument('--input_size', default=224, type=int, help='images input size')\n",
    "\n",
    "    # training parameters\n",
    "    parser.add_argument(\"--train_config\", default='train_configs.json', type=str, help='training configurations')\n",
    "    parser.add_argument('--mask', action='store_true')\n",
    "    parser.set_defaults(mask=True)\n",
    "    parser.add_argument('--model_ema_decay', type=float, default=0.9998, help='')\n",
    "    parser.add_argument('--model_ema_force_cpu', action='store_true', default=False, help='')\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n",
    "                        help='Optimizer (default: \"adamw\"')\n",
    "    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n",
    "                        help='Optimizer Epsilon (default: 1e-8)')\n",
    "    parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                        help='Optimizer Betas (default: None, use opt default)')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='weight decay (default: 0.05)')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate (default: 1e-3)')\n",
    "    parser.add_argument('--layer_decay', type=float, default=0.65)\n",
    "    parser.add_argument('--warmup_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='warmup learning rate (default: 1e-6)')\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=0, metavar='N',\n",
    "                        help='epochs to warmup LR, if scheduler supports')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n",
    "                        help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n",
    "\n",
    "    # Augmentation parameters\n",
    "    parser.add_argument('--train_crop_min', default=0.3, type=float)\n",
    "    parser.add_argument('--color_jitter', type=float, default=0, metavar='PCT')\n",
    "    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n",
    "                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "    parser.add_argument('--train_interpolation', type=str, default='bicubic',\n",
    "                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--nb_classes', default=0, type=int, help='number of the classification types')\n",
    "    parser.add_argument('--dataset', default='imagenet', type=str, help='dataset name')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='', help='path to save checkpoint and log')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='',\n",
    "                        help='resume from checkpoint')\n",
    "    parser.add_argument('--auto_resume', action='store_true')\n",
    "    parser.set_defaults(auto_resume=True)\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true',\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--amp', action='store_true')\n",
    "\n",
    "    return parser.parse_args()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opts = get_args()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    utils.init_distributed_mode(args)\n",
    "\n",
    "    train_configs = json.load(open(args.train_config,'r'))\n",
    "    train_config = train_configs[args.dataset+'_'+args.clip_model]\n",
    "\n",
    "    if not args.output_dir:\n",
    "        args.output_dir = os.path.join('output',args.dataset)\n",
    "        if args.mask:\n",
    "            args.output_dir = os.path.join(args.output_dir, \"%s_mpatch%d_mratio%.1f_walign%.1f_tau%.1f_epoch%d_lr%.5f\"%(args.clip_model[:5],train_config['mask_patch_size'],train_config['mask_ratio'],train_config['w_align'],train_config['conf_threshold'],train_config['epochs'], train_config['lr']))\n",
    "        else:\n",
    "            args.output_dir = os.path.join(args.output_dir, \"%s_tau%.1f_epoch%d_lr%.5f\"%(args.clip_model[:5],train_config['conf_threshold'],train_config['epochs'], train_config['lr']))\n",
    "\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # create image classifier from pretrained clip model\n",
    "    model = clip_classifier(args)\n",
    "    args.nb_classes = len(model.classnames)\n",
    "\n",
    "    dataset_train = build_dataset(is_train=True, args=args, train_config=train_config)\n",
    "    dataset_val = build_dataset(is_train=False, args=args)\n",
    "\n",
    "    if True:  # args.distributed:\n",
    "        num_tasks = utils.get_world_size()\n",
    "        global_rank = utils.get_rank()\n",
    "        sampler_train = torch.utils.data.DistributedSampler(\n",
    "            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
    "        )\n",
    "        print(\"Sampler_train = %s\" % str(sampler_train))\n",
    "    else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    if global_rank == 0 and args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "        log_writer = utils.TensorboardLogger(log_dir=args.output_dir)\n",
    "    else:\n",
    "        log_writer = None\n",
    "    if args.output_dir and utils.is_main_process():\n",
    "        with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(dict(args._get_kwargs())) + \"\\n\")\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, sampler=sampler_val,\n",
    "        batch_size=2*args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    model_ema = ModelEma(\n",
    "        model,\n",
    "        decay=args.model_ema_decay,\n",
    "        resume='')\n",
    "    print(\"Using EMA with decay = %.5f\" % (args.model_ema_decay) )\n",
    "\n",
    "    model_without_ddp = model\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"Model = %s\" % str(model_without_ddp))\n",
    "    print('number of params:', n_parameters)\n",
    "\n",
    "    total_batch_size = args.batch_size * utils.get_world_size()\n",
    "    num_training_steps_per_epoch = len(data_loader_train)\n",
    "\n",
    "    args.lr = train_config['lr'] * total_batch_size / 256\n",
    "    args.min_lr = args.min_lr * total_batch_size / 256\n",
    "    args.epochs = train_config['epochs']\n",
    "    args.eval_freq = train_config['eval_freq']\n",
    "    print(\"LR = %.8f\" % args.lr)\n",
    "    print(\"Batch size = %d\" % total_batch_size)\n",
    "    print(\"Number of training examples = %d\" % len(dataset_train))\n",
    "\n",
    "    num_layers = model_without_ddp.model.visual.transformer.layers\n",
    "    if args.layer_decay < 1.0:\n",
    "        assigner = LayerDecayValueAssigner(list(args.layer_decay ** (num_layers + 1 - i) for i in range(num_layers + 2)))\n",
    "    else:\n",
    "        assigner = None\n",
    "\n",
    "    if assigner is not None:\n",
    "        print(\"Assigned values = %s\" % str(assigner.values))\n",
    "\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module\n",
    "\n",
    "    optimizer = create_optimizer(\n",
    "        args, model_without_ddp,\n",
    "        get_num_layer=assigner.get_layer_id if assigner is not None else None,\n",
    "        get_layer_scale=assigner.get_scale if assigner is not None else None)\n",
    "    if args.amp:\n",
    "        loss_scaler = NativeScaler()\n",
    "        amp_autocast = torch.cuda.amp.autocast\n",
    "    else:\n",
    "        loss_scaler = None\n",
    "        amp_autocast = suppress\n",
    "\n",
    "    lr_schedule_values = utils.cosine_scheduler(\n",
    "        args.lr, args.min_lr, args.epochs, num_training_steps_per_epoch,\n",
    "        warmup_epochs=args.warmup_epochs, warmup_steps=args.warmup_steps,\n",
    "    )\n",
    "\n",
    "    utils.auto_load_model(\n",
    "        args=args, model=model, model_without_ddp=model_without_ddp,\n",
    "        optimizer=optimizer, loss_scaler=loss_scaler, model_ema=model_ema)\n",
    "\n",
    "    if args.eval:\n",
    "        test_stats = evaluate(data_loader_val, model, device, args=args)\n",
    "        print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
    "        exit(0)\n",
    "\n",
    "\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "    start_time = time.time()\n",
    "    max_accuracy = 0.0\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            data_loader_train.sampler.set_epoch(epoch)\n",
    "        if log_writer is not None:\n",
    "            log_writer.set_step(epoch * num_training_steps_per_epoch)\n",
    "\n",
    "        train_stats = train_one_epoch(\n",
    "            model, args, train_config,\n",
    "            data_loader_train, optimizer, amp_autocast, device, epoch, loss_scaler,\n",
    "            log_writer=log_writer,\n",
    "            start_steps=epoch * num_training_steps_per_epoch,\n",
    "            lr_schedule_values=lr_schedule_values,\n",
    "            model_ema=model_ema,\n",
    "        )\n",
    "\n",
    "        if args.output_dir and utils.is_main_process() and (epoch + 1) % args.eval_freq == 0:\n",
    "            if (epoch + 1) % args.save_ckpt_freq == 0 or epoch + 1 == args.epochs:\n",
    "                utils.save_model(\n",
    "                    args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n",
    "                    loss_scaler=loss_scaler, epoch=epoch, model_ema=model_ema)\n",
    "\n",
    "            test_stats = evaluate(data_loader_val, model, device, model_ema=model_ema, args=args)\n",
    "            print(f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n",
    "            if max_accuracy < test_stats[\"acc1\"]:\n",
    "                max_accuracy = test_stats[\"acc1\"]\n",
    "                if args.output_dir:\n",
    "                    utils.save_model(\n",
    "                        args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n",
    "                        loss_scaler=loss_scaler, epoch=\"best\", model_ema=model_ema)\n",
    "\n",
    "            print(f'Max accuracy: {max_accuracy:.2f}%')\n",
    "            if log_writer is not None:\n",
    "                log_writer.update(test_acc1=test_stats['acc1'], head=\"test\", step=epoch)\n",
    "                log_writer.update(test_ema_acc1=test_stats['ema_acc1'], head=\"test\", step=epoch)\n",
    "                log_writer.flush()\n",
    "\n",
    "            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                         **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                         'epoch': epoch,\n",
    "                         'n_parameters': n_parameters}\n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}